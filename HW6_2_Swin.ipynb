{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO0Grj7TfKnjc8zwx0fWutU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myllanes/Introduction-to-Deep-Learning/blob/main/HW6_2_Swin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz5jwbQK4VGg",
        "outputId": "0005e683-c21f-4a3a-9d1d-dc185f0a799b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable Parameters:\n",
            "\tclassifier.conv.weight\n",
            "\tclassifier.conv.bias\n",
            "\tclassifier.bn.weight\n",
            "\tclassifier.bn.bias\n",
            "\tclassifier.fc.weight\n",
            "\tclassifier.fc.bias\n",
            "=== Training with Conv-Padded Classifier Head ===\n",
            "Using device: cuda\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:13<00:00, 11.69it/s, loss=3.6, acc=36.0%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "Time: 0:02:13.685298\n",
            "Train Accuracy: 35.97%\n",
            "Avg Loss: 3.5953\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:13<00:00, 11.71it/s, loss=2.48, acc=59.4%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Time: 0:02:13.461502\n",
            "Train Accuracy: 59.39%\n",
            "Avg Loss: 2.4824\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 313/313 [00:26<00:00, 11.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Final Results ===\n",
            "Total Training Time: 0:04:27.146800\n",
            "Average Epoch Time: 0:02:13.573400\n",
            "\n",
            "Training Accuracies by Epoch:\n",
            "Epoch 1: 35.97% (Time: 0:02:13.685298)\n",
            "Epoch 2: 59.39% (Time: 0:02:13.461502)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Michael Yllanes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import SwinForImageClassification, AutoImageProcessor\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "learning_rate = 2e-5\n",
        "image_size = 224\n",
        "num_classes = 100\n",
        "hidden_dim = 768  # hidden dimension\n",
        "\n",
        "# Data preprocessing\n",
        "processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-small-patch4-window7-224\") # Change with microsoft/swin-tiny-patch4-window7-224 or microsoft/swin-small-patch4-window7-224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
        "])\n",
        "\n",
        "# Load CIFAR-100\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Classifier\n",
        "class classifierH(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(hidden_dim, hidden_dim//2, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(hidden_dim//2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(hidden_dim//2, num_classes)\n",
        "        # Zero-padding\n",
        "        self.pad = nn.ZeroPad2d((0, 0, 0, max(0, 100 - num_classes)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        x = self.fc(x)\n",
        "        return self.pad(x)\n",
        "\n",
        "# Load model with custom head\n",
        "model = SwinForImageClassification.from_pretrained(\n",
        "    \"microsoft/swin-tiny-patch4-window7-224\",\n",
        "    num_labels=num_classes,\n",
        "    ignore_mismatched_sizes=True\n",
        ").to(device)\n",
        "\n",
        "# Replace classifier\n",
        "model.classifier = classifierH(hidden_dim, num_classes).to(device)\n",
        "\n",
        "# Freeze backbone, train only head for fine tune\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"\\nTrainable Parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"\\t{name}\")\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    epoch_start = time.time()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': total_loss/(progress_bar.n+1),\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    epoch_acc = 100 * correct / total\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    return epoch_time, epoch_acc, avg_loss\n",
        "\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_time = time.time() - start_time\n",
        "    accuracy = 100 * correct / total\n",
        "    return test_time, accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=== Training with Conv-Padded Classifier Head ===\")\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # Training loop\n",
        "    train_times = []\n",
        "    train_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_time, epoch_acc, epoch_loss = train_epoch(epoch)\n",
        "        train_times.append(epoch_time)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "        print(f\"Time: {timedelta(seconds=epoch_time)}\")\n",
        "        print(f\"Train Accuracy: {epoch_acc:.2f}%\")\n",
        "        print(f\"Avg Loss: {epoch_loss:.4f}\\n\")\n",
        "\n",
        "    # Final evaluation\n",
        "    test_time, test_acc = evaluate()\n",
        "\n",
        "    # Results summary\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    print(f\"Total Training Time: {timedelta(seconds=sum(train_times))}\")\n",
        "    print(f\"Average Epoch Time: {timedelta(seconds=sum(train_times)/num_epochs)}\")\n",
        "    print(\"\\nTraining Accuracies by Epoch:\")\n",
        "    for i, acc in enumerate(train_accs):\n",
        "        print(f\"Epoch {i+1}: {acc:.2f}% (Time: {timedelta(seconds=train_times[i])})\")\n"
      ]
    }
  ]
}