{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNOaKPrLSjhBOAhHjNWY6/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myllanes/Introduction-to-Deep-Learning/blob/main/HW5_2_2_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lJ6_FkQxx9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b7711e-fec5-4ce2-d2b7-c74410893b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 567913\n",
            "Epoch 1, Batch 0, Loss: 4.4009\n",
            "Epoch 1, Batch 100, Loss: 2.2543\n",
            "Epoch 1, Batch 200, Loss: 2.6117\n",
            "Epoch 1, Batch 300, Loss: 2.5063\n"
          ]
        }
      ],
      "source": [
        "#Michael Yllanes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Download the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # This is the entire text data\n",
        "\n",
        "# Prepare the dataset\n",
        "sequence_length = 20 # Sequence\n",
        "\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "# Dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)\n",
        "\n",
        "# Data loaders\n",
        "batch_size = 128\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Encoding\n",
        "class Encoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(Encoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Transformer model\n",
        "class CharTransformer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, nhead, num_layers):\n",
        "        super(CharTransformer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.pos_encoder = Encoding(hidden_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding and positional encoding\n",
        "        embedded = self.embedding(x) * math.sqrt(self.hidden_size)\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "\n",
        "        # Transformer expects\n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "\n",
        "        # Transformer encoder\n",
        "        transformer_out = self.transformer_encoder(embedded)\n",
        "\n",
        "        # Get the last time step's output\n",
        "        output = self.fc(transformer_out[-1, :, :])\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(chars)\n",
        "hidden_size = 120\n",
        "output_size = len(chars)\n",
        "learning_rate = 0.002\n",
        "epochs = 5\n",
        "\n",
        "# Model\n",
        "model = CharTransformer(input_size, hidden_size, output_size, nhead=2, num_layers=1) # Number of Heads and Layers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Calculate model size\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n",
        "\n",
        "# Training metrics storage\n",
        "training_metrics = {\n",
        "    'epoch_loss': [],\n",
        "    'epoch_accuracy': [],\n",
        "    'batch_loss': [],\n",
        "    'training_time': 0\n",
        "}\n",
        "\n",
        "validation_metrics = {\n",
        "    'loss': 0,\n",
        "    'accuracy': 0\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "def train(model, train_loader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            training_metrics['batch_loss'].append(loss.item())\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "        training_metrics['epoch_loss'].append(avg_loss)\n",
        "        training_metrics['epoch_accuracy'].append(accuracy)\n",
        "        print(f\"Epoch {epoch+1}, Average Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    training_metrics['training_time'] = time.time() - start_time\n",
        "    print(f\"Training time: {training_metrics['training_time']:.2f} seconds\")\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total_samples += target.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / total_samples\n",
        "    validation_metrics['loss'] = avg_loss\n",
        "    validation_metrics['accuracy'] = accuracy\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, test_loader, criterion)\n",
        "\n",
        "# Generate the final report\n",
        "print(f\"Training Duration: {training_metrics['training_time']:.2f} seconds\")\n",
        "print(f\"Final Training Loss: {training_metrics['epoch_loss'][-1]:.4f}\")\n",
        "print(f\"Final Training Accuracy: {training_metrics['epoch_accuracy'][-1]:.4f}\")\n",
        "print(f\"Validation Loss: {validation_metrics['loss']:.4f}\")\n",
        "print(f\"Validation Accuracy: {validation_metrics['accuracy']:.4f}\")\n",
        "print(f\"Model Parameters: {total_params}\")\n",
        "\n",
        "# Computational Complexity Analysis\n",
        "print(f\"  - Embedding layer: ({input_size * hidden_size})\")\n",
        "print(f\"  - Final linear layer: ({hidden_size * output_size})\")"
      ]
    }
  ]
}