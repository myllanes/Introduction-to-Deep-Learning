{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOjeDZsCD8rclYok8dUM5Uq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myllanes/Introduction-to-Deep-Learning/blob/main/H3_2_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lJ6_FkQxx9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6e5b55-3fb5-40c2-960c-4397faa1bdd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 202457\n",
            "Epoch 1, Batch 0, Loss: 4.1781\n",
            "Epoch 1, Batch 100, Loss: 2.8763\n",
            "Epoch 1, Batch 200, Loss: 2.7952\n",
            "Epoch 1, Batch 300, Loss: 2.6637\n",
            "Epoch 1, Batch 400, Loss: 2.4456\n",
            "Epoch 1, Batch 500, Loss: 2.3883\n",
            "Epoch 1, Batch 600, Loss: 2.3291\n",
            "Epoch 1, Batch 700, Loss: 2.3152\n",
            "Epoch 1, Batch 800, Loss: 2.3480\n",
            "Epoch 1, Batch 900, Loss: 2.1869\n",
            "Epoch 1, Batch 1000, Loss: 2.1859\n",
            "Epoch 1, Batch 1100, Loss: 2.2481\n",
            "Epoch 1, Batch 1200, Loss: 2.1379\n",
            "Epoch 1, Batch 1300, Loss: 2.2436\n",
            "Epoch 1, Batch 1400, Loss: 2.2986\n",
            "Epoch 1, Batch 1500, Loss: 2.2792\n",
            "Epoch 1, Batch 1600, Loss: 2.2307\n",
            "Epoch 1, Batch 1700, Loss: 2.1393\n",
            "Epoch 1, Batch 1800, Loss: 2.0327\n",
            "Epoch 1, Batch 1900, Loss: 2.0777\n",
            "Epoch 1, Batch 2000, Loss: 2.1244\n",
            "Epoch 1, Batch 2100, Loss: 2.0284\n",
            "Epoch 1, Batch 2200, Loss: 2.0780\n",
            "Epoch 1, Batch 2300, Loss: 1.9897\n",
            "Epoch 1, Batch 2400, Loss: 2.3778\n",
            "Epoch 1, Batch 2500, Loss: 1.9482\n",
            "Epoch 1, Batch 2600, Loss: 2.3155\n",
            "Epoch 1, Batch 2700, Loss: 2.2034\n",
            "Epoch 1, Batch 2800, Loss: 1.9845\n",
            "Epoch 1, Batch 2900, Loss: 2.2539\n",
            "Epoch 1, Batch 3000, Loss: 1.9369\n",
            "Epoch 1, Batch 3100, Loss: 2.0479\n",
            "Epoch 1, Batch 3200, Loss: 2.1412\n",
            "Epoch 1, Batch 3300, Loss: 2.0091\n",
            "Epoch 1, Batch 3400, Loss: 1.9125\n",
            "Epoch 1, Batch 3500, Loss: 1.8928\n",
            "Epoch 1, Batch 3600, Loss: 1.9394\n",
            "Epoch 1, Batch 3700, Loss: 2.0822\n",
            "Epoch 1, Batch 3800, Loss: 1.9857\n",
            "Epoch 1, Batch 3900, Loss: 1.9011\n",
            "Epoch 1, Batch 4000, Loss: 1.8551\n",
            "Epoch 1, Batch 4100, Loss: 1.9315\n",
            "Epoch 1, Batch 4200, Loss: 1.8315\n",
            "Epoch 1, Batch 4300, Loss: 2.0546\n",
            "Epoch 1, Batch 4400, Loss: 2.1329\n",
            "Epoch 1, Batch 4500, Loss: 2.2331\n",
            "Epoch 1, Batch 4600, Loss: 2.2547\n",
            "Epoch 1, Batch 4700, Loss: 2.0994\n",
            "Epoch 1, Batch 4800, Loss: 1.8910\n",
            "Epoch 1, Batch 4900, Loss: 1.9851\n",
            "Epoch 1, Batch 5000, Loss: 2.0463\n",
            "Epoch 1, Batch 5100, Loss: 2.1346\n",
            "Epoch 1, Batch 5200, Loss: 1.9397\n",
            "Epoch 1, Batch 5300, Loss: 2.2141\n",
            "Epoch 1, Batch 5400, Loss: 1.8439\n",
            "Epoch 1, Batch 5500, Loss: 2.0923\n",
            "Epoch 1, Batch 5600, Loss: 1.8765\n",
            "Epoch 1, Batch 5700, Loss: 2.0163\n",
            "Epoch 1, Batch 5800, Loss: 2.0503\n",
            "Epoch 1, Batch 5900, Loss: 1.8153\n",
            "Epoch 1, Batch 6000, Loss: 1.8889\n",
            "Epoch 1, Batch 6100, Loss: 2.0192\n",
            "Epoch 1, Batch 6200, Loss: 1.8548\n",
            "Epoch 1, Batch 6300, Loss: 2.2213\n",
            "Epoch 1, Batch 6400, Loss: 1.9228\n",
            "Epoch 1, Batch 6500, Loss: 2.1391\n",
            "Epoch 1, Batch 6600, Loss: 2.1805\n",
            "Epoch 1, Batch 6700, Loss: 1.8994\n",
            "Epoch 1, Batch 6800, Loss: 1.8347\n",
            "Epoch 1, Batch 6900, Loss: 1.7608\n",
            "Epoch 1, Average Training Loss: 2.1158, Training Accuracy: 0.3938\n",
            "Epoch 2, Batch 0, Loss: 1.7410\n",
            "Epoch 2, Batch 100, Loss: 2.0525\n",
            "Epoch 2, Batch 200, Loss: 2.0264\n",
            "Epoch 2, Batch 300, Loss: 1.8569\n",
            "Epoch 2, Batch 400, Loss: 2.2172\n",
            "Epoch 2, Batch 500, Loss: 1.7946\n",
            "Epoch 2, Batch 600, Loss: 1.9956\n",
            "Epoch 2, Batch 700, Loss: 1.8269\n",
            "Epoch 2, Batch 800, Loss: 1.9535\n",
            "Epoch 2, Batch 900, Loss: 1.8500\n",
            "Epoch 2, Batch 1000, Loss: 1.9483\n",
            "Epoch 2, Batch 1100, Loss: 1.9601\n",
            "Epoch 2, Batch 1200, Loss: 2.0308\n",
            "Epoch 2, Batch 1300, Loss: 1.8915\n",
            "Epoch 2, Batch 1400, Loss: 1.7568\n",
            "Epoch 2, Batch 1500, Loss: 1.8860\n",
            "Epoch 2, Batch 1600, Loss: 1.9602\n",
            "Epoch 2, Batch 1700, Loss: 1.8527\n",
            "Epoch 2, Batch 1800, Loss: 1.9263\n",
            "Epoch 2, Batch 1900, Loss: 1.9558\n",
            "Epoch 2, Batch 2000, Loss: 1.9612\n",
            "Epoch 2, Batch 2100, Loss: 1.9006\n",
            "Epoch 2, Batch 2200, Loss: 1.8259\n",
            "Epoch 2, Batch 2300, Loss: 1.8079\n",
            "Epoch 2, Batch 2400, Loss: 2.0355\n",
            "Epoch 2, Batch 2500, Loss: 2.0281\n",
            "Epoch 2, Batch 2600, Loss: 1.8320\n",
            "Epoch 2, Batch 2700, Loss: 1.7004\n",
            "Epoch 2, Batch 2800, Loss: 1.7759\n",
            "Epoch 2, Batch 2900, Loss: 1.7326\n",
            "Epoch 2, Batch 3000, Loss: 1.8085\n",
            "Epoch 2, Batch 3100, Loss: 1.8195\n",
            "Epoch 2, Batch 3200, Loss: 1.7816\n",
            "Epoch 2, Batch 3300, Loss: 1.8100\n",
            "Epoch 2, Batch 3400, Loss: 1.9018\n",
            "Epoch 2, Batch 3500, Loss: 1.6648\n",
            "Epoch 2, Batch 3600, Loss: 1.8330\n",
            "Epoch 2, Batch 3700, Loss: 1.9180\n",
            "Epoch 2, Batch 3800, Loss: 1.9477\n",
            "Epoch 2, Batch 3900, Loss: 2.1147\n",
            "Epoch 2, Batch 4000, Loss: 1.6955\n",
            "Epoch 2, Batch 4100, Loss: 1.9510\n",
            "Epoch 2, Batch 4200, Loss: 1.9008\n",
            "Epoch 2, Batch 4300, Loss: 1.9263\n",
            "Epoch 2, Batch 4400, Loss: 1.8624\n",
            "Epoch 2, Batch 4500, Loss: 1.8663\n",
            "Epoch 2, Batch 4600, Loss: 2.0042\n",
            "Epoch 2, Batch 4700, Loss: 1.7963\n",
            "Epoch 2, Batch 4800, Loss: 1.9259\n",
            "Epoch 2, Batch 4900, Loss: 1.8323\n",
            "Epoch 2, Batch 5000, Loss: 1.9557\n",
            "Epoch 2, Batch 5100, Loss: 1.7337\n",
            "Epoch 2, Batch 5200, Loss: 1.8844\n",
            "Epoch 2, Batch 5300, Loss: 1.9000\n",
            "Epoch 2, Batch 5400, Loss: 2.0651\n",
            "Epoch 2, Batch 5500, Loss: 1.9761\n",
            "Epoch 2, Batch 5600, Loss: 2.3045\n",
            "Epoch 2, Batch 5700, Loss: 1.8089\n",
            "Epoch 2, Batch 5800, Loss: 1.7807\n",
            "Epoch 2, Batch 5900, Loss: 1.8788\n",
            "Epoch 2, Batch 6000, Loss: 1.8172\n",
            "Epoch 2, Batch 6100, Loss: 1.6879\n",
            "Epoch 2, Batch 6200, Loss: 1.9293\n",
            "Epoch 2, Batch 6300, Loss: 1.7380\n",
            "Epoch 2, Batch 6400, Loss: 2.0682\n",
            "Epoch 2, Batch 6500, Loss: 1.8239\n",
            "Epoch 2, Batch 6600, Loss: 1.9359\n",
            "Epoch 2, Batch 6700, Loss: 1.7732\n",
            "Epoch 2, Batch 6800, Loss: 1.7248\n",
            "Epoch 2, Batch 6900, Loss: 1.9974\n",
            "Epoch 2, Average Training Loss: 1.9033, Training Accuracy: 0.4445\n",
            "Epoch 3, Batch 0, Loss: 1.7709\n",
            "Epoch 3, Batch 100, Loss: 1.8319\n",
            "Epoch 3, Batch 200, Loss: 1.9007\n",
            "Epoch 3, Batch 300, Loss: 1.8321\n",
            "Epoch 3, Batch 400, Loss: 1.8627\n",
            "Epoch 3, Batch 500, Loss: 1.9146\n",
            "Epoch 3, Batch 600, Loss: 1.8093\n",
            "Epoch 3, Batch 700, Loss: 1.9881\n",
            "Epoch 3, Batch 800, Loss: 1.8264\n",
            "Epoch 3, Batch 900, Loss: 1.7801\n",
            "Epoch 3, Batch 1000, Loss: 1.9636\n",
            "Epoch 3, Batch 1100, Loss: 1.7646\n",
            "Epoch 3, Batch 1200, Loss: 1.8586\n",
            "Epoch 3, Batch 1300, Loss: 1.9517\n",
            "Epoch 3, Batch 1400, Loss: 1.9328\n",
            "Epoch 3, Batch 1500, Loss: 1.6097\n",
            "Epoch 3, Batch 1600, Loss: 1.8748\n",
            "Epoch 3, Batch 1700, Loss: 1.8759\n",
            "Epoch 3, Batch 1800, Loss: 1.8394\n",
            "Epoch 3, Batch 1900, Loss: 1.9182\n",
            "Epoch 3, Batch 2000, Loss: 1.7305\n",
            "Epoch 3, Batch 2100, Loss: 1.7810\n",
            "Epoch 3, Batch 2200, Loss: 1.9983\n",
            "Epoch 3, Batch 2300, Loss: 1.9515\n",
            "Epoch 3, Batch 2400, Loss: 1.9755\n",
            "Epoch 3, Batch 2500, Loss: 1.6557\n",
            "Epoch 3, Batch 2600, Loss: 1.8278\n",
            "Epoch 3, Batch 2700, Loss: 1.7772\n",
            "Epoch 3, Batch 2800, Loss: 1.6627\n",
            "Epoch 3, Batch 2900, Loss: 1.6327\n",
            "Epoch 3, Batch 3000, Loss: 1.6771\n",
            "Epoch 3, Batch 3100, Loss: 1.7421\n",
            "Epoch 3, Batch 3200, Loss: 1.8986\n",
            "Epoch 3, Batch 3300, Loss: 1.9388\n",
            "Epoch 3, Batch 3400, Loss: 1.8985\n",
            "Epoch 3, Batch 3500, Loss: 1.9425\n",
            "Epoch 3, Batch 3600, Loss: 1.6352\n",
            "Epoch 3, Batch 3700, Loss: 1.8929\n",
            "Epoch 3, Batch 3800, Loss: 1.7530\n",
            "Epoch 3, Batch 3900, Loss: 1.6121\n",
            "Epoch 3, Batch 4000, Loss: 1.8214\n",
            "Epoch 3, Batch 4100, Loss: 1.9069\n",
            "Epoch 3, Batch 4200, Loss: 1.9431\n",
            "Epoch 3, Batch 4300, Loss: 1.7972\n",
            "Epoch 3, Batch 4400, Loss: 1.7395\n",
            "Epoch 3, Batch 4500, Loss: 1.7788\n",
            "Epoch 3, Batch 4600, Loss: 1.8148\n",
            "Epoch 3, Batch 4700, Loss: 1.5520\n",
            "Epoch 3, Batch 4800, Loss: 1.8388\n",
            "Epoch 3, Batch 4900, Loss: 1.9253\n",
            "Epoch 3, Batch 5000, Loss: 1.8436\n",
            "Epoch 3, Batch 5100, Loss: 1.9203\n",
            "Epoch 3, Batch 5200, Loss: 2.0941\n",
            "Epoch 3, Batch 5300, Loss: 1.9993\n",
            "Epoch 3, Batch 5400, Loss: 1.9091\n",
            "Epoch 3, Batch 5500, Loss: 1.9351\n",
            "Epoch 3, Batch 5600, Loss: 1.9208\n",
            "Epoch 3, Batch 5700, Loss: 1.6336\n",
            "Epoch 3, Batch 5800, Loss: 1.9293\n",
            "Epoch 3, Batch 5900, Loss: 1.9905\n",
            "Epoch 3, Batch 6000, Loss: 1.9364\n",
            "Epoch 3, Batch 6100, Loss: 1.8737\n",
            "Epoch 3, Batch 6200, Loss: 1.9057\n",
            "Epoch 3, Batch 6300, Loss: 1.7411\n",
            "Epoch 3, Batch 6400, Loss: 1.9222\n",
            "Epoch 3, Batch 6500, Loss: 1.8525\n",
            "Epoch 3, Batch 6600, Loss: 1.7212\n",
            "Epoch 3, Batch 6700, Loss: 1.9257\n",
            "Epoch 3, Batch 6800, Loss: 1.9113\n",
            "Epoch 3, Batch 6900, Loss: 1.8565\n",
            "Epoch 3, Average Training Loss: 1.8426, Training Accuracy: 0.4604\n",
            "Epoch 4, Batch 0, Loss: 2.0517\n",
            "Epoch 4, Batch 100, Loss: 1.9290\n",
            "Epoch 4, Batch 200, Loss: 1.6331\n",
            "Epoch 4, Batch 300, Loss: 1.6605\n",
            "Epoch 4, Batch 400, Loss: 1.7613\n",
            "Epoch 4, Batch 500, Loss: 1.9145\n",
            "Epoch 4, Batch 600, Loss: 1.6831\n",
            "Epoch 4, Batch 700, Loss: 2.1160\n",
            "Epoch 4, Batch 800, Loss: 1.6413\n",
            "Epoch 4, Batch 900, Loss: 1.8954\n",
            "Epoch 4, Batch 1000, Loss: 1.8699\n",
            "Epoch 4, Batch 1100, Loss: 2.0134\n",
            "Epoch 4, Batch 1200, Loss: 1.9047\n",
            "Epoch 4, Batch 1300, Loss: 1.7358\n",
            "Epoch 4, Batch 1400, Loss: 1.7882\n",
            "Epoch 4, Batch 1500, Loss: 1.7139\n",
            "Epoch 4, Batch 1600, Loss: 1.9758\n",
            "Epoch 4, Batch 1700, Loss: 1.5299\n",
            "Epoch 4, Batch 1800, Loss: 2.0334\n",
            "Epoch 4, Batch 1900, Loss: 1.9466\n",
            "Epoch 4, Batch 2000, Loss: 1.6936\n",
            "Epoch 4, Batch 2100, Loss: 1.7337\n",
            "Epoch 4, Batch 2200, Loss: 1.7815\n",
            "Epoch 4, Batch 2300, Loss: 1.7829\n",
            "Epoch 4, Batch 2400, Loss: 1.9595\n",
            "Epoch 4, Batch 2500, Loss: 1.5382\n",
            "Epoch 4, Batch 2600, Loss: 1.7773\n",
            "Epoch 4, Batch 2700, Loss: 1.6227\n",
            "Epoch 4, Batch 2800, Loss: 1.7908\n",
            "Epoch 4, Batch 2900, Loss: 2.0183\n",
            "Epoch 4, Batch 3000, Loss: 1.9062\n",
            "Epoch 4, Batch 3100, Loss: 1.9390\n",
            "Epoch 4, Batch 3200, Loss: 1.6019\n",
            "Epoch 4, Batch 3300, Loss: 1.7777\n",
            "Epoch 4, Batch 3400, Loss: 1.8123\n",
            "Epoch 4, Batch 3500, Loss: 1.7858\n",
            "Epoch 4, Batch 3600, Loss: 1.8603\n",
            "Epoch 4, Batch 3700, Loss: 1.8499\n",
            "Epoch 4, Batch 3800, Loss: 1.6770\n",
            "Epoch 4, Batch 3900, Loss: 1.7653\n",
            "Epoch 4, Batch 4000, Loss: 1.8038\n",
            "Epoch 4, Batch 4100, Loss: 1.7488\n",
            "Epoch 4, Batch 4200, Loss: 1.7224\n",
            "Epoch 4, Batch 4300, Loss: 1.6279\n",
            "Epoch 4, Batch 4400, Loss: 1.9041\n",
            "Epoch 4, Batch 4500, Loss: 1.9357\n",
            "Epoch 4, Batch 4600, Loss: 1.7899\n",
            "Epoch 4, Batch 4700, Loss: 1.9140\n",
            "Epoch 4, Batch 4800, Loss: 1.6488\n",
            "Epoch 4, Batch 4900, Loss: 1.9557\n",
            "Epoch 4, Batch 5000, Loss: 2.1179\n",
            "Epoch 4, Batch 5100, Loss: 2.0525\n",
            "Epoch 4, Batch 5200, Loss: 1.5638\n",
            "Epoch 4, Batch 5300, Loss: 1.8455\n",
            "Epoch 4, Batch 5400, Loss: 1.6147\n",
            "Epoch 4, Batch 5500, Loss: 1.8087\n",
            "Epoch 4, Batch 5600, Loss: 1.7073\n",
            "Epoch 4, Batch 5700, Loss: 1.6810\n",
            "Epoch 4, Batch 5800, Loss: 1.8343\n",
            "Epoch 4, Batch 5900, Loss: 2.0214\n",
            "Epoch 4, Batch 6000, Loss: 1.8439\n",
            "Epoch 4, Batch 6100, Loss: 1.8891\n",
            "Epoch 4, Batch 6200, Loss: 1.8113\n",
            "Epoch 4, Batch 6300, Loss: 2.0800\n",
            "Epoch 4, Batch 6400, Loss: 1.6891\n",
            "Epoch 4, Batch 6500, Loss: 1.6729\n",
            "Epoch 4, Batch 6600, Loss: 1.6898\n",
            "Epoch 4, Batch 6700, Loss: 1.7618\n",
            "Epoch 4, Batch 6800, Loss: 1.7862\n",
            "Epoch 4, Batch 6900, Loss: 1.8152\n",
            "Epoch 4, Average Training Loss: 1.8153, Training Accuracy: 0.4671\n",
            "Epoch 5, Batch 0, Loss: 1.7791\n",
            "Epoch 5, Batch 100, Loss: 1.9267\n",
            "Epoch 5, Batch 200, Loss: 1.7534\n",
            "Epoch 5, Batch 300, Loss: 1.8352\n",
            "Epoch 5, Batch 400, Loss: 1.8604\n",
            "Epoch 5, Batch 500, Loss: 1.8352\n",
            "Epoch 5, Batch 600, Loss: 1.7139\n",
            "Epoch 5, Batch 700, Loss: 1.7348\n",
            "Epoch 5, Batch 800, Loss: 1.9199\n",
            "Epoch 5, Batch 900, Loss: 1.5479\n",
            "Epoch 5, Batch 1000, Loss: 1.7234\n",
            "Epoch 5, Batch 1100, Loss: 1.7438\n",
            "Epoch 5, Batch 1200, Loss: 1.7914\n",
            "Epoch 5, Batch 1300, Loss: 1.6771\n",
            "Epoch 5, Batch 1400, Loss: 1.9709\n",
            "Epoch 5, Batch 1500, Loss: 1.9350\n",
            "Epoch 5, Batch 1600, Loss: 1.7280\n",
            "Epoch 5, Batch 1700, Loss: 1.6639\n",
            "Epoch 5, Batch 1800, Loss: 1.8011\n",
            "Epoch 5, Batch 1900, Loss: 1.7771\n",
            "Epoch 5, Batch 2000, Loss: 1.7284\n",
            "Epoch 5, Batch 2100, Loss: 1.9352\n",
            "Epoch 5, Batch 2200, Loss: 1.6174\n",
            "Epoch 5, Batch 2300, Loss: 1.7653\n",
            "Epoch 5, Batch 2400, Loss: 1.8928\n",
            "Epoch 5, Batch 2500, Loss: 1.7023\n",
            "Epoch 5, Batch 2600, Loss: 1.6838\n",
            "Epoch 5, Batch 2700, Loss: 1.6470\n",
            "Epoch 5, Batch 2800, Loss: 1.6183\n",
            "Epoch 5, Batch 2900, Loss: 1.6538\n",
            "Epoch 5, Batch 3000, Loss: 1.8118\n",
            "Epoch 5, Batch 3100, Loss: 1.6957\n",
            "Epoch 5, Batch 3200, Loss: 1.7188\n",
            "Epoch 5, Batch 3300, Loss: 1.5910\n",
            "Epoch 5, Batch 3400, Loss: 1.8575\n",
            "Epoch 5, Batch 3500, Loss: 1.9922\n",
            "Epoch 5, Batch 3600, Loss: 1.7614\n",
            "Epoch 5, Batch 3700, Loss: 1.6750\n",
            "Epoch 5, Batch 3800, Loss: 1.8111\n",
            "Epoch 5, Batch 3900, Loss: 1.9804\n",
            "Epoch 5, Batch 4000, Loss: 1.8182\n",
            "Epoch 5, Batch 4100, Loss: 1.8158\n",
            "Epoch 5, Batch 4200, Loss: 1.5946\n",
            "Epoch 5, Batch 4300, Loss: 1.7392\n",
            "Epoch 5, Batch 4400, Loss: 1.7670\n",
            "Epoch 5, Batch 4500, Loss: 1.9021\n",
            "Epoch 5, Batch 4600, Loss: 1.7429\n",
            "Epoch 5, Batch 4700, Loss: 1.6713\n",
            "Epoch 5, Batch 4800, Loss: 1.7950\n",
            "Epoch 5, Batch 4900, Loss: 1.6812\n",
            "Epoch 5, Batch 5000, Loss: 1.7412\n",
            "Epoch 5, Batch 5100, Loss: 1.5108\n",
            "Epoch 5, Batch 5200, Loss: 1.6970\n",
            "Epoch 5, Batch 5300, Loss: 1.9510\n",
            "Epoch 5, Batch 5400, Loss: 1.8379\n",
            "Epoch 5, Batch 5500, Loss: 1.8276\n",
            "Epoch 5, Batch 5600, Loss: 1.7924\n",
            "Epoch 5, Batch 5700, Loss: 1.6717\n",
            "Epoch 5, Batch 5800, Loss: 1.7294\n",
            "Epoch 5, Batch 5900, Loss: 1.7523\n",
            "Epoch 5, Batch 6000, Loss: 1.7543\n",
            "Epoch 5, Batch 6100, Loss: 1.7513\n",
            "Epoch 5, Batch 6200, Loss: 1.9357\n",
            "Epoch 5, Batch 6300, Loss: 1.8582\n",
            "Epoch 5, Batch 6400, Loss: 1.7277\n",
            "Epoch 5, Batch 6500, Loss: 1.5852\n",
            "Epoch 5, Batch 6600, Loss: 1.8522\n",
            "Epoch 5, Batch 6700, Loss: 1.6686\n",
            "Epoch 5, Batch 6800, Loss: 1.8808\n",
            "Epoch 5, Batch 6900, Loss: 1.8028\n",
            "Epoch 5, Average Training Loss: 1.7972, Training Accuracy: 0.4712\n",
            "Epoch 6, Batch 0, Loss: 1.7004\n",
            "Epoch 6, Batch 100, Loss: 1.7462\n",
            "Epoch 6, Batch 200, Loss: 2.0322\n",
            "Epoch 6, Batch 300, Loss: 1.8166\n",
            "Epoch 6, Batch 400, Loss: 1.8969\n",
            "Epoch 6, Batch 500, Loss: 1.6844\n",
            "Epoch 6, Batch 600, Loss: 1.9837\n",
            "Epoch 6, Batch 700, Loss: 1.7598\n",
            "Epoch 6, Batch 800, Loss: 1.7067\n",
            "Epoch 6, Batch 900, Loss: 1.6841\n",
            "Epoch 6, Batch 1000, Loss: 1.8035\n",
            "Epoch 6, Batch 1100, Loss: 1.8312\n",
            "Epoch 6, Batch 1200, Loss: 1.6266\n",
            "Epoch 6, Batch 1300, Loss: 1.8603\n",
            "Epoch 6, Batch 1400, Loss: 1.6258\n",
            "Epoch 6, Batch 1500, Loss: 1.8926\n",
            "Epoch 6, Batch 1600, Loss: 1.8638\n",
            "Epoch 6, Batch 1700, Loss: 1.8394\n",
            "Epoch 6, Batch 1800, Loss: 1.7217\n",
            "Epoch 6, Batch 1900, Loss: 1.5433\n",
            "Epoch 6, Batch 2000, Loss: 1.6905\n",
            "Epoch 6, Batch 2100, Loss: 1.8718\n",
            "Epoch 6, Batch 2200, Loss: 1.8324\n",
            "Epoch 6, Batch 2300, Loss: 1.8962\n",
            "Epoch 6, Batch 2400, Loss: 1.8040\n",
            "Epoch 6, Batch 2500, Loss: 1.8629\n",
            "Epoch 6, Batch 2600, Loss: 1.5287\n",
            "Epoch 6, Batch 2700, Loss: 1.8111\n",
            "Epoch 6, Batch 2800, Loss: 1.8317\n",
            "Epoch 6, Batch 2900, Loss: 1.8032\n",
            "Epoch 6, Batch 3000, Loss: 1.6916\n",
            "Epoch 6, Batch 3100, Loss: 1.7635\n",
            "Epoch 6, Batch 3200, Loss: 1.9505\n",
            "Epoch 6, Batch 3300, Loss: 1.7027\n",
            "Epoch 6, Batch 3400, Loss: 1.9201\n",
            "Epoch 6, Batch 3500, Loss: 1.6587\n",
            "Epoch 6, Batch 3600, Loss: 1.5391\n",
            "Epoch 6, Batch 3700, Loss: 1.8612\n",
            "Epoch 6, Batch 3800, Loss: 1.5901\n",
            "Epoch 6, Batch 3900, Loss: 2.0594\n",
            "Epoch 6, Batch 4000, Loss: 1.6600\n",
            "Epoch 6, Batch 4100, Loss: 1.8934\n",
            "Epoch 6, Batch 4200, Loss: 1.6155\n",
            "Epoch 6, Batch 4300, Loss: 1.7201\n",
            "Epoch 6, Batch 4400, Loss: 1.9335\n",
            "Epoch 6, Batch 4500, Loss: 1.8129\n",
            "Epoch 6, Batch 4600, Loss: 2.1867\n",
            "Epoch 6, Batch 4700, Loss: 1.7308\n",
            "Epoch 6, Batch 4800, Loss: 1.6591\n",
            "Epoch 6, Batch 4900, Loss: 1.8366\n",
            "Epoch 6, Batch 5000, Loss: 1.6418\n",
            "Epoch 6, Batch 5100, Loss: 1.6535\n",
            "Epoch 6, Batch 5200, Loss: 1.8376\n",
            "Epoch 6, Batch 5300, Loss: 1.8585\n",
            "Epoch 6, Batch 5400, Loss: 1.7182\n",
            "Epoch 6, Batch 5500, Loss: 1.5431\n",
            "Epoch 6, Batch 5600, Loss: 1.8567\n",
            "Epoch 6, Batch 5700, Loss: 1.6672\n",
            "Epoch 6, Batch 5800, Loss: 1.8509\n",
            "Epoch 6, Batch 5900, Loss: 1.8286\n",
            "Epoch 6, Batch 6000, Loss: 1.6906\n",
            "Epoch 6, Batch 6100, Loss: 1.8529\n",
            "Epoch 6, Batch 6200, Loss: 1.7921\n",
            "Epoch 6, Batch 6300, Loss: 1.8182\n",
            "Epoch 6, Batch 6400, Loss: 1.6883\n",
            "Epoch 6, Batch 6500, Loss: 1.8329\n",
            "Epoch 6, Batch 6600, Loss: 1.7765\n",
            "Epoch 6, Batch 6700, Loss: 1.8802\n",
            "Epoch 6, Batch 6800, Loss: 1.8341\n",
            "Epoch 6, Batch 6900, Loss: 1.8634\n",
            "Epoch 6, Average Training Loss: 1.7844, Training Accuracy: 0.4743\n",
            "Epoch 7, Batch 0, Loss: 1.7989\n",
            "Epoch 7, Batch 100, Loss: 1.8994\n",
            "Epoch 7, Batch 200, Loss: 1.9243\n",
            "Epoch 7, Batch 300, Loss: 1.9400\n",
            "Epoch 7, Batch 400, Loss: 1.6359\n",
            "Epoch 7, Batch 500, Loss: 1.7420\n",
            "Epoch 7, Batch 600, Loss: 1.7846\n",
            "Epoch 7, Batch 700, Loss: 1.9544\n",
            "Epoch 7, Batch 800, Loss: 1.9844\n",
            "Epoch 7, Batch 900, Loss: 1.7607\n",
            "Epoch 7, Batch 1000, Loss: 1.7964\n",
            "Epoch 7, Batch 1100, Loss: 1.9687\n",
            "Epoch 7, Batch 1200, Loss: 1.7674\n",
            "Epoch 7, Batch 1300, Loss: 1.7275\n",
            "Epoch 7, Batch 1400, Loss: 1.8194\n",
            "Epoch 7, Batch 1500, Loss: 1.9072\n",
            "Epoch 7, Batch 1600, Loss: 1.7323\n",
            "Epoch 7, Batch 1700, Loss: 1.8280\n",
            "Epoch 7, Batch 1800, Loss: 1.8471\n",
            "Epoch 7, Batch 1900, Loss: 1.7915\n",
            "Epoch 7, Batch 2000, Loss: 1.8668\n",
            "Epoch 7, Batch 2100, Loss: 1.9438\n",
            "Epoch 7, Batch 2200, Loss: 1.8427\n",
            "Epoch 7, Batch 2300, Loss: 1.6111\n",
            "Epoch 7, Batch 2400, Loss: 1.9105\n",
            "Epoch 7, Batch 2500, Loss: 1.5223\n",
            "Epoch 7, Batch 2600, Loss: 1.5762\n",
            "Epoch 7, Batch 2700, Loss: 1.6809\n",
            "Epoch 7, Batch 2800, Loss: 1.8792\n",
            "Epoch 7, Batch 2900, Loss: 1.5999\n",
            "Epoch 7, Batch 3000, Loss: 1.9032\n",
            "Epoch 7, Batch 3100, Loss: 1.6648\n",
            "Epoch 7, Batch 3200, Loss: 1.7006\n",
            "Epoch 7, Batch 3300, Loss: 1.8368\n",
            "Epoch 7, Batch 3400, Loss: 1.7569\n",
            "Epoch 7, Batch 3500, Loss: 1.8152\n",
            "Epoch 7, Batch 3600, Loss: 1.8216\n",
            "Epoch 7, Batch 3700, Loss: 1.7630\n",
            "Epoch 7, Batch 3800, Loss: 1.6810\n",
            "Epoch 7, Batch 3900, Loss: 1.8427\n",
            "Epoch 7, Batch 4000, Loss: 1.8520\n",
            "Epoch 7, Batch 4100, Loss: 1.7598\n",
            "Epoch 7, Batch 4200, Loss: 1.8122\n",
            "Epoch 7, Batch 4300, Loss: 1.7686\n",
            "Epoch 7, Batch 4400, Loss: 1.8348\n",
            "Epoch 7, Batch 4500, Loss: 1.6707\n",
            "Epoch 7, Batch 4600, Loss: 1.6847\n",
            "Epoch 7, Batch 4700, Loss: 1.9142\n",
            "Epoch 7, Batch 4800, Loss: 1.8858\n",
            "Epoch 7, Batch 4900, Loss: 1.9398\n",
            "Epoch 7, Batch 5000, Loss: 1.8838\n",
            "Epoch 7, Batch 5100, Loss: 1.4706\n",
            "Epoch 7, Batch 5200, Loss: 1.7163\n",
            "Epoch 7, Batch 5300, Loss: 2.0158\n",
            "Epoch 7, Batch 5400, Loss: 1.7723\n",
            "Epoch 7, Batch 5500, Loss: 1.8634\n",
            "Epoch 7, Batch 5600, Loss: 1.7373\n",
            "Epoch 7, Batch 5700, Loss: 1.7778\n",
            "Epoch 7, Batch 5800, Loss: 1.5542\n",
            "Epoch 7, Batch 5900, Loss: 1.9238\n",
            "Epoch 7, Batch 6000, Loss: 1.8334\n",
            "Epoch 7, Batch 6100, Loss: 1.8236\n",
            "Epoch 7, Batch 6200, Loss: 1.9043\n",
            "Epoch 7, Batch 6300, Loss: 1.6805\n",
            "Epoch 7, Batch 6400, Loss: 1.5512\n",
            "Epoch 7, Batch 6500, Loss: 1.7467\n",
            "Epoch 7, Batch 6600, Loss: 1.8893\n",
            "Epoch 7, Batch 6700, Loss: 1.7313\n",
            "Epoch 7, Batch 6800, Loss: 1.7005\n",
            "Epoch 7, Batch 6900, Loss: 1.6970\n",
            "Epoch 7, Average Training Loss: 1.7752, Training Accuracy: 0.4767\n",
            "Epoch 8, Batch 0, Loss: 1.9275\n",
            "Epoch 8, Batch 100, Loss: 1.7273\n",
            "Epoch 8, Batch 200, Loss: 1.6576\n",
            "Epoch 8, Batch 300, Loss: 1.6604\n",
            "Epoch 8, Batch 400, Loss: 1.8655\n",
            "Epoch 8, Batch 500, Loss: 1.9364\n",
            "Epoch 8, Batch 600, Loss: 1.7572\n",
            "Epoch 8, Batch 700, Loss: 1.8538\n",
            "Epoch 8, Batch 800, Loss: 1.6896\n",
            "Epoch 8, Batch 900, Loss: 1.8620\n",
            "Epoch 8, Batch 1000, Loss: 1.8645\n",
            "Epoch 8, Batch 1100, Loss: 1.6311\n",
            "Epoch 8, Batch 1200, Loss: 1.7553\n",
            "Epoch 8, Batch 1300, Loss: 1.6083\n",
            "Epoch 8, Batch 1400, Loss: 1.7694\n",
            "Epoch 8, Batch 1500, Loss: 1.7212\n",
            "Epoch 8, Batch 1600, Loss: 1.6862\n",
            "Epoch 8, Batch 1700, Loss: 1.5702\n",
            "Epoch 8, Batch 1800, Loss: 1.8180\n",
            "Epoch 8, Batch 1900, Loss: 1.7152\n",
            "Epoch 8, Batch 2000, Loss: 1.6566\n",
            "Epoch 8, Batch 2100, Loss: 1.6916\n",
            "Epoch 8, Batch 2200, Loss: 1.8124\n",
            "Epoch 8, Batch 2300, Loss: 1.7587\n",
            "Epoch 8, Batch 2400, Loss: 1.9709\n",
            "Epoch 8, Batch 2500, Loss: 1.9392\n",
            "Epoch 8, Batch 2600, Loss: 1.8912\n",
            "Epoch 8, Batch 2700, Loss: 1.8499\n",
            "Epoch 8, Batch 2800, Loss: 1.7626\n",
            "Epoch 8, Batch 2900, Loss: 1.6185\n",
            "Epoch 8, Batch 3000, Loss: 1.6947\n",
            "Epoch 8, Batch 3100, Loss: 1.7505\n",
            "Epoch 8, Batch 3200, Loss: 1.8007\n",
            "Epoch 8, Batch 3300, Loss: 1.8687\n",
            "Epoch 8, Batch 3400, Loss: 1.6841\n",
            "Epoch 8, Batch 3500, Loss: 1.7671\n",
            "Epoch 8, Batch 3600, Loss: 1.6909\n",
            "Epoch 8, Batch 3700, Loss: 1.5347\n",
            "Epoch 8, Batch 3800, Loss: 1.6869\n",
            "Epoch 8, Batch 3900, Loss: 1.6584\n",
            "Epoch 8, Batch 4000, Loss: 1.6811\n",
            "Epoch 8, Batch 4100, Loss: 1.8667\n",
            "Epoch 8, Batch 4200, Loss: 1.7949\n",
            "Epoch 8, Batch 4300, Loss: 1.6535\n",
            "Epoch 8, Batch 4400, Loss: 2.0106\n",
            "Epoch 8, Batch 4500, Loss: 1.6683\n",
            "Epoch 8, Batch 4600, Loss: 1.6335\n",
            "Epoch 8, Batch 4700, Loss: 1.8561\n",
            "Epoch 8, Batch 4800, Loss: 1.7850\n",
            "Epoch 8, Batch 4900, Loss: 1.6840\n",
            "Epoch 8, Batch 5000, Loss: 1.7830\n",
            "Epoch 8, Batch 5100, Loss: 1.9562\n",
            "Epoch 8, Batch 5200, Loss: 1.6690\n",
            "Epoch 8, Batch 5300, Loss: 1.6921\n",
            "Epoch 8, Batch 5400, Loss: 1.4925\n",
            "Epoch 8, Batch 5500, Loss: 1.8498\n",
            "Epoch 8, Batch 5600, Loss: 1.7601\n",
            "Epoch 8, Batch 5700, Loss: 1.6393\n",
            "Epoch 8, Batch 5800, Loss: 1.7349\n",
            "Epoch 8, Batch 5900, Loss: 1.8356\n",
            "Epoch 8, Batch 6000, Loss: 2.0622\n",
            "Epoch 8, Batch 6100, Loss: 1.8192\n",
            "Epoch 8, Batch 6200, Loss: 1.7690\n",
            "Epoch 8, Batch 6300, Loss: 1.4887\n",
            "Epoch 8, Batch 6400, Loss: 1.7906\n",
            "Epoch 8, Batch 6500, Loss: 1.9311\n",
            "Epoch 8, Batch 6600, Loss: 2.1028\n",
            "Epoch 8, Batch 6700, Loss: 1.7069\n",
            "Epoch 8, Batch 6800, Loss: 1.6546\n",
            "Epoch 8, Batch 6900, Loss: 1.8954\n",
            "Epoch 8, Average Training Loss: 1.7690, Training Accuracy: 0.4781\n",
            "Epoch 9, Batch 0, Loss: 1.7493\n",
            "Epoch 9, Batch 100, Loss: 1.6705\n",
            "Epoch 9, Batch 200, Loss: 1.6907\n",
            "Epoch 9, Batch 300, Loss: 1.8434\n",
            "Epoch 9, Batch 400, Loss: 1.7761\n",
            "Epoch 9, Batch 500, Loss: 1.7052\n",
            "Epoch 9, Batch 600, Loss: 1.7290\n",
            "Epoch 9, Batch 700, Loss: 2.0257\n",
            "Epoch 9, Batch 800, Loss: 1.6052\n",
            "Epoch 9, Batch 900, Loss: 1.8065\n",
            "Epoch 9, Batch 1000, Loss: 1.7710\n",
            "Epoch 9, Batch 1100, Loss: 1.8140\n",
            "Epoch 9, Batch 1200, Loss: 1.6646\n",
            "Epoch 9, Batch 1300, Loss: 1.8167\n",
            "Epoch 9, Batch 1400, Loss: 1.8364\n",
            "Epoch 9, Batch 1500, Loss: 1.9401\n",
            "Epoch 9, Batch 1600, Loss: 1.6854\n",
            "Epoch 9, Batch 1700, Loss: 1.6963\n",
            "Epoch 9, Batch 1800, Loss: 1.9476\n",
            "Epoch 9, Batch 1900, Loss: 1.9575\n",
            "Epoch 9, Batch 2000, Loss: 1.6486\n",
            "Epoch 9, Batch 2100, Loss: 1.7400\n",
            "Epoch 9, Batch 2200, Loss: 1.9820\n",
            "Epoch 9, Batch 2300, Loss: 1.8195\n",
            "Epoch 9, Batch 2400, Loss: 1.7459\n",
            "Epoch 9, Batch 2500, Loss: 1.7064\n",
            "Epoch 9, Batch 2600, Loss: 1.9817\n",
            "Epoch 9, Batch 2700, Loss: 1.9573\n",
            "Epoch 9, Batch 2800, Loss: 1.4426\n",
            "Epoch 9, Batch 2900, Loss: 1.8362\n",
            "Epoch 9, Batch 3000, Loss: 1.9820\n",
            "Epoch 9, Batch 3100, Loss: 1.7018\n",
            "Epoch 9, Batch 3200, Loss: 1.7438\n",
            "Epoch 9, Batch 3300, Loss: 1.7653\n",
            "Epoch 9, Batch 3400, Loss: 1.5882\n",
            "Epoch 9, Batch 3500, Loss: 1.8308\n",
            "Epoch 9, Batch 3600, Loss: 1.6251\n",
            "Epoch 9, Batch 3700, Loss: 1.5709\n",
            "Epoch 9, Batch 3800, Loss: 1.8206\n",
            "Epoch 9, Batch 3900, Loss: 1.9014\n",
            "Epoch 9, Batch 4000, Loss: 1.9002\n",
            "Epoch 9, Batch 4100, Loss: 1.5040\n",
            "Epoch 9, Batch 4200, Loss: 1.4220\n",
            "Epoch 9, Batch 4300, Loss: 1.7866\n",
            "Epoch 9, Batch 4400, Loss: 1.8757\n",
            "Epoch 9, Batch 4500, Loss: 1.7444\n",
            "Epoch 9, Batch 4600, Loss: 1.6129\n",
            "Epoch 9, Batch 4700, Loss: 1.9180\n",
            "Epoch 9, Batch 4800, Loss: 1.7200\n",
            "Epoch 9, Batch 4900, Loss: 1.6912\n",
            "Epoch 9, Batch 5000, Loss: 1.9241\n",
            "Epoch 9, Batch 5100, Loss: 1.9874\n",
            "Epoch 9, Batch 5200, Loss: 1.8083\n",
            "Epoch 9, Batch 5300, Loss: 1.6686\n",
            "Epoch 9, Batch 5400, Loss: 1.7347\n",
            "Epoch 9, Batch 5500, Loss: 1.7327\n",
            "Epoch 9, Batch 5600, Loss: 1.7680\n",
            "Epoch 9, Batch 5700, Loss: 1.6255\n",
            "Epoch 9, Batch 5800, Loss: 1.8859\n",
            "Epoch 9, Batch 5900, Loss: 1.9175\n",
            "Epoch 9, Batch 6000, Loss: 1.7390\n",
            "Epoch 9, Batch 6100, Loss: 1.7527\n",
            "Epoch 9, Batch 6200, Loss: 1.9330\n",
            "Epoch 9, Batch 6300, Loss: 1.6251\n",
            "Epoch 9, Batch 6400, Loss: 1.5124\n",
            "Epoch 9, Batch 6500, Loss: 1.8429\n",
            "Epoch 9, Batch 6600, Loss: 1.7885\n",
            "Epoch 9, Batch 6700, Loss: 1.9744\n",
            "Epoch 9, Batch 6800, Loss: 1.8524\n",
            "Epoch 9, Batch 6900, Loss: 2.0309\n",
            "Epoch 9, Average Training Loss: 1.7638, Training Accuracy: 0.4800\n",
            "Epoch 10, Batch 0, Loss: 1.5707\n",
            "Epoch 10, Batch 100, Loss: 1.5773\n",
            "Epoch 10, Batch 200, Loss: 1.9941\n",
            "Epoch 10, Batch 300, Loss: 1.5562\n",
            "Epoch 10, Batch 400, Loss: 1.6432\n",
            "Epoch 10, Batch 500, Loss: 1.6931\n",
            "Epoch 10, Batch 600, Loss: 1.8508\n",
            "Epoch 10, Batch 700, Loss: 1.5231\n",
            "Epoch 10, Batch 800, Loss: 1.8351\n",
            "Epoch 10, Batch 900, Loss: 1.6576\n",
            "Epoch 10, Batch 1000, Loss: 1.6400\n",
            "Epoch 10, Batch 1100, Loss: 1.6921\n",
            "Epoch 10, Batch 1200, Loss: 1.8273\n",
            "Epoch 10, Batch 1300, Loss: 1.5422\n",
            "Epoch 10, Batch 1400, Loss: 1.6552\n",
            "Epoch 10, Batch 1500, Loss: 1.5850\n",
            "Epoch 10, Batch 1600, Loss: 1.7921\n",
            "Epoch 10, Batch 1700, Loss: 1.6435\n",
            "Epoch 10, Batch 1800, Loss: 2.0533\n",
            "Epoch 10, Batch 1900, Loss: 1.6086\n",
            "Epoch 10, Batch 2000, Loss: 1.4041\n",
            "Epoch 10, Batch 2100, Loss: 1.9047\n",
            "Epoch 10, Batch 2200, Loss: 1.6589\n",
            "Epoch 10, Batch 2300, Loss: 1.7113\n",
            "Epoch 10, Batch 2400, Loss: 1.9016\n",
            "Epoch 10, Batch 2500, Loss: 1.7191\n",
            "Epoch 10, Batch 2600, Loss: 1.7307\n",
            "Epoch 10, Batch 2700, Loss: 1.6785\n",
            "Epoch 10, Batch 2800, Loss: 1.5968\n",
            "Epoch 10, Batch 2900, Loss: 1.8456\n",
            "Epoch 10, Batch 3000, Loss: 1.7241\n",
            "Epoch 10, Batch 3100, Loss: 1.8010\n",
            "Epoch 10, Batch 3200, Loss: 1.8204\n",
            "Epoch 10, Batch 3300, Loss: 1.9207\n",
            "Epoch 10, Batch 3400, Loss: 1.6237\n",
            "Epoch 10, Batch 3500, Loss: 1.7916\n",
            "Epoch 10, Batch 3600, Loss: 1.8203\n",
            "Epoch 10, Batch 3700, Loss: 1.6343\n",
            "Epoch 10, Batch 3800, Loss: 1.4716\n",
            "Epoch 10, Batch 3900, Loss: 1.7269\n",
            "Epoch 10, Batch 4000, Loss: 1.8779\n",
            "Epoch 10, Batch 4100, Loss: 1.8869\n",
            "Epoch 10, Batch 4200, Loss: 1.8264\n",
            "Epoch 10, Batch 4300, Loss: 1.6935\n",
            "Epoch 10, Batch 4400, Loss: 1.9595\n",
            "Epoch 10, Batch 4500, Loss: 1.7688\n",
            "Epoch 10, Batch 4600, Loss: 1.9488\n",
            "Epoch 10, Batch 4700, Loss: 1.8461\n",
            "Epoch 10, Batch 4800, Loss: 1.7779\n",
            "Epoch 10, Batch 4900, Loss: 1.9003\n",
            "Epoch 10, Batch 5000, Loss: 1.7220\n",
            "Epoch 10, Batch 5100, Loss: 1.8843\n",
            "Epoch 10, Batch 5200, Loss: 1.8908\n",
            "Epoch 10, Batch 5300, Loss: 1.6645\n",
            "Epoch 10, Batch 5400, Loss: 1.5560\n",
            "Epoch 10, Batch 5500, Loss: 2.0416\n",
            "Epoch 10, Batch 5600, Loss: 1.6428\n",
            "Epoch 10, Batch 5700, Loss: 1.8300\n",
            "Epoch 10, Batch 5800, Loss: 1.9371\n",
            "Epoch 10, Batch 5900, Loss: 1.9524\n",
            "Epoch 10, Batch 6000, Loss: 1.6354\n",
            "Epoch 10, Batch 6100, Loss: 1.9853\n",
            "Epoch 10, Batch 6200, Loss: 1.8655\n",
            "Epoch 10, Batch 6300, Loss: 1.7570\n",
            "Epoch 10, Batch 6400, Loss: 1.5283\n",
            "Epoch 10, Batch 6500, Loss: 1.8243\n",
            "Epoch 10, Batch 6600, Loss: 1.9836\n",
            "Epoch 10, Batch 6700, Loss: 1.8521\n",
            "Epoch 10, Batch 6800, Loss: 1.7081\n",
            "Epoch 10, Batch 6900, Loss: 1.7693\n",
            "Epoch 10, Average Training Loss: 1.7584, Training Accuracy: 0.4807\n",
            "Training time: 3392.00 seconds\n",
            "Validation Loss: 1.5630, Validation Accuracy: 0.5253\n",
            "Total trainable parameters: 202457\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Download the dataset\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text  # This is the entire text data\n",
        "\n",
        "# Step 2: Prepare the dataset\n",
        "sequence_length = 50\n",
        "# Create a character mapping to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encode the text into integers\n",
        "encoded_text = [char_to_int[ch] for ch in text]\n",
        "\n",
        "# Create sequences and targets\n",
        "sequences = []\n",
        "targets = []\n",
        "for i in range(0, len(encoded_text) - sequence_length):\n",
        "    seq = encoded_text[i:i+sequence_length]\n",
        "    target = encoded_text[i+sequence_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "# Step 3: Create a dataset class\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.sequences[index], self.targets[index]\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = CharDataset(sequences, targets)\n",
        "\n",
        "# Step 4: Create data loaders\n",
        "batch_size = 128\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# LSTM model\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm1 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward pass through LSTM layers\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm1(embedded, (h0, c0))\n",
        "        lstm_out = self.dropout1(lstm_out)\n",
        "        lstm_out, _ = self.lstm2(lstm_out, (h0, c0))\n",
        "        lstm_out = self.dropout2(lstm_out)\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(chars)  # Number of unique characters\n",
        "hidden_size = 108\n",
        "output_size = len(chars)  # Number of unique characters\n",
        "learning_rate = 0.002\n",
        "epochs = 10\n",
        "\n",
        "# Model, loss, and optimizer\n",
        "model = CharLSTM(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Calculate model size\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n",
        "\n",
        "# Step 6: Training loop\n",
        "def train(model, train_loader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Average Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Step 7: Evaluation\n",
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate(model, test_loader, criterion)\n",
        "\n",
        "# Report model size and execution time\n",
        "print(f\"Total trainable parameters: {total_params}\")\n"
      ]
    }
  ]
}