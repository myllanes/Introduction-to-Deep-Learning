{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOJDV0CoTrrJEeZD+detfWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myllanes/Introduction-to-Deep-Learning/blob/main/HW6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vknqjf3TBAF5",
        "outputId": "df8ee2e2-283e-4055-9867-979da4f46795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 29.72it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 79.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 14.16s\n",
            "Train Loss: 4.3289 | Train Acc: 5.21%\n",
            "Val Loss: 4.2980 | Val Acc: 5.81%\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 29.96it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 83.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 14.00s\n",
            "Train Loss: 4.2578 | Train Acc: 5.98%\n",
            "Val Loss: 4.2228 | Val Acc: 6.58%\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.37it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 84.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.81s\n",
            "Train Loss: 4.2300 | Train Acc: 6.20%\n",
            "Val Loss: 4.1690 | Val Acc: 7.79%\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.75it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 84.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.66s\n",
            "Train Loss: 4.1798 | Train Acc: 7.00%\n",
            "Val Loss: 4.1357 | Val Acc: 7.59%\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.79it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 85.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.63s\n",
            "Train Loss: 4.1431 | Train Acc: 7.66%\n",
            "Val Loss: 4.1572 | Val Acc: 8.03%\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.76it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:01<00:00, 78.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.72s\n",
            "Train Loss: 4.1295 | Train Acc: 7.89%\n",
            "Val Loss: 4.1445 | Val Acc: 8.42%\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.56it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 82.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.76s\n",
            "Train Loss: 4.1201 | Train Acc: 8.23%\n",
            "Val Loss: 4.1088 | Val Acc: 8.65%\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.53it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 81.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.78s\n",
            "Train Loss: 4.1026 | Train Acc: 8.52%\n",
            "Val Loss: 4.0840 | Val Acc: 8.64%\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.53it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 82.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.76s\n",
            "Train Loss: 4.0709 | Train Acc: 8.95%\n",
            "Val Loss: 4.0895 | Val Acc: 8.98%\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.63it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 83.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.72s\n",
            "Train Loss: 4.0558 | Train Acc: 9.33%\n",
            "Val Loss: 4.0766 | Val Acc: 8.98%\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.69it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 81.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.71s\n",
            "Train Loss: 4.0283 | Train Acc: 9.80%\n",
            "Val Loss: 4.0129 | Val Acc: 10.04%\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.71it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 84.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.67s\n",
            "Train Loss: 4.0123 | Train Acc: 10.15%\n",
            "Val Loss: 3.9961 | Val Acc: 10.68%\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.71it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 81.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.71s\n",
            "Train Loss: 3.9855 | Train Acc: 10.43%\n",
            "Val Loss: 3.9803 | Val Acc: 10.91%\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.66it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 84.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.69s\n",
            "Train Loss: 3.9670 | Train Acc: 11.31%\n",
            "Val Loss: 3.9804 | Val Acc: 11.03%\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.63it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 83.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.72s\n",
            "Train Loss: 3.9594 | Train Acc: 11.10%\n",
            "Val Loss: 3.9549 | Val Acc: 11.48%\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.62it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 80.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.76s\n",
            "Train Loss: 3.9348 | Train Acc: 11.58%\n",
            "Val Loss: 3.9399 | Val Acc: 11.69%\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.62it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 83.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.71s\n",
            "Train Loss: 3.9156 | Train Acc: 12.07%\n",
            "Val Loss: 3.9332 | Val Acc: 12.01%\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.60it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 83.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.73s\n",
            "Train Loss: 3.9043 | Train Acc: 12.30%\n",
            "Val Loss: 3.9324 | Val Acc: 12.27%\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.61it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 82.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.74s\n",
            "Train Loss: 3.8951 | Train Acc: 12.37%\n",
            "Val Loss: 3.9318 | Val Acc: 12.04%\n",
            "\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [00:12<00:00, 30.66it/s]\n",
            "Evaluating: 100%|██████████| 79/79 [00:00<00:00, 84.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 13.70s\n",
            "Train Loss: 3.8950 | Train Acc: 12.54%\n",
            "Val Loss: 3.9301 | Val Acc: 12.13%\n",
            "Total Training Time: 275.14 seconds\n",
            "Average Time per Epoch: 13.76 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Michael Yllanes\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "image_size = 32       # CIFAR image size\n",
        "patch_size = 8        # 4x4 patches\n",
        "num_classes = 100     # 100 classes for CIFAR-100\n",
        "embed_dim = 512       # Embedding dimension\n",
        "num_heads = 4         # Attention heads\n",
        "num_layers = 4        # Transformer layers\n",
        "mlp_ratio = 4         # MLP hidden dim = embed_dim * mlp_ratio\n",
        "dropout = 0.1         # Dropout rate\n",
        "batch_size = 128      # Batch size\n",
        "num_epochs = 20      # CIFAR-100\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.03   # Weight decay\n",
        "\n",
        "# CIFAR-100\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 stats\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Converts images into patch embeddings with convolutional projection\"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim,\n",
        "            kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # [B, embed_dim, H', W']\n",
        "        x = x.flatten(2)  # [B, embed_dim, num_patches]\n",
        "        x = x.transpose(1, 2)  # [B, num_patches, embed_dim]\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block with pre-norm architecture\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio, dropout):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        # MLP with residual connection\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Complete ViT model for CIFAR-100 classification\"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, num_classes=100,\n",
        "                 embed_dim=256, num_heads=2, num_layers=4,\n",
        "                 mlp_ratio=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.zeros_(m.bias)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Classification\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "# Initialize model\n",
        "model = VisionTransformer(\n",
        "    img_size=image_size,\n",
        "    patch_size=patch_size,\n",
        "    num_classes=num_classes,\n",
        "    embed_dim=embed_dim,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    mlp_ratio=mlp_ratio,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "# Loss function with label smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_samples = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training loop with validation and timing\n",
        "best_acc = 0.0\n",
        "total_training_time = 0.0  # total training time\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "overall_start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate epoch time\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    total_training_time += epoch_time\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Time: {epoch_time:.2f}s\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# Calculate total training time\n",
        "total_time = time.time() - overall_start_time\n",
        "\n",
        "# Print final results and timing information\n",
        "print(f\"Total Training Time: {total_time:.2f} seconds\")\n",
        "print(f\"Average Time per Epoch: {total_time/num_epochs:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import time\n",
        "\n",
        "# Using torchinfo\n",
        "# Model summary\n",
        "summary(model, input_size=(batch_size, 3, 32, 32),\n",
        "       verbose=1, col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
        "\n",
        "# Timing\n",
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "repetitions = 100\n",
        "timings = []\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = model(torch.randn(batch_size, 3, 32, 32).to(device))\n",
        "\n",
        "# Measurement\n",
        "with torch.no_grad():\n",
        "    for _ in range(repetitions):\n",
        "        inputs = torch.randn(batch_size, 3, 32, 32).to(device)\n",
        "        starter.record()\n",
        "        _ = model(inputs)\n",
        "        ender.record()\n",
        "        torch.cuda.synchronize()\n",
        "        timings.append(starter.elapsed_time(ender))\n",
        "\n",
        "avg_time = sum(timings) / repetitions\n",
        "print(f\"Average forward pass time: {avg_time:.2f}ms\")\n",
        "print(f\"Throughput: {1000/(avg_time)*batch_size:.0f} samples/sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t771xzJUpawu",
        "outputId": "c1172d76-8e74-420c-c8f0-eb342eb9c272"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "============================================================================================================================================\n",
            "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Mult-Adds\n",
            "============================================================================================================================================\n",
            "VisionTransformer                        [128, 3, 32, 32]          [128, 100]                9,216                     --\n",
            "├─PatchEmbedding: 1-1                    [128, 3, 32, 32]          [128, 16, 512]            --                        --\n",
            "│    └─Conv2d: 2-1                       [128, 3, 32, 32]          [128, 512, 4, 4]          98,816                    202,375,168\n",
            "├─Dropout: 1-2                           [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "├─Sequential: 1-3                        [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "│    └─TransformerBlock: 2-2             [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "│    │    └─LayerNorm: 3-1               [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─LayerNorm: 3-2               [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─LayerNorm: 3-3               [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─MultiheadAttention: 3-4      [128, 17, 512]            [128, 17, 512]            1,050,624                 0\n",
            "│    │    └─LayerNorm: 3-5               [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─Sequential: 3-6              [128, 17, 512]            [128, 17, 512]            2,099,712                 268,763,136\n",
            "│    └─TransformerBlock: 2-3             [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "│    │    └─LayerNorm: 3-7               [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─LayerNorm: 3-8               [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─LayerNorm: 3-9               [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─MultiheadAttention: 3-10     [128, 17, 512]            [128, 17, 512]            1,050,624                 0\n",
            "│    │    └─LayerNorm: 3-11              [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─Sequential: 3-12             [128, 17, 512]            [128, 17, 512]            2,099,712                 268,763,136\n",
            "│    └─TransformerBlock: 2-4             [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "│    │    └─LayerNorm: 3-13              [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─LayerNorm: 3-14              [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─LayerNorm: 3-15              [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─MultiheadAttention: 3-16     [128, 17, 512]            [128, 17, 512]            1,050,624                 0\n",
            "│    │    └─LayerNorm: 3-17              [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─Sequential: 3-18             [128, 17, 512]            [128, 17, 512]            2,099,712                 268,763,136\n",
            "│    └─TransformerBlock: 2-5             [128, 17, 512]            [128, 17, 512]            --                        --\n",
            "│    │    └─LayerNorm: 3-19              [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─LayerNorm: 3-20              [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─LayerNorm: 3-21              [128, 17, 512]            [128, 17, 512]            (recursive)               131,072\n",
            "│    │    └─MultiheadAttention: 3-22     [128, 17, 512]            [128, 17, 512]            1,050,624                 0\n",
            "│    │    └─LayerNorm: 3-23              [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "│    │    └─Sequential: 3-24             [128, 17, 512]            [128, 17, 512]            2,099,712                 268,763,136\n",
            "├─LayerNorm: 1-4                         [128, 17, 512]            [128, 17, 512]            1,024                     131,072\n",
            "├─Linear: 1-5                            [128, 512]                [128, 100]                51,300                    6,566,400\n",
            "============================================================================================================================================\n",
            "Total params: 12,769,892\n",
            "Trainable params: 12,769,892\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 1.29\n",
            "============================================================================================================================================\n",
            "Input size (MB): 1.57\n",
            "Forward/backward pass size (MB): 338.27\n",
            "Params size (MB): 34.23\n",
            "Estimated Total Size (MB): 374.07\n",
            "============================================================================================================================================\n",
            "Average forward pass time: 8.68ms\n",
            "Throughput: 14744 samples/sec\n"
          ]
        }
      ]
    }
  ]
}